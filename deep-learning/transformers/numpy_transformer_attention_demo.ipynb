{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4692b58f",
   "metadata": {},
   "source": [
    "\n",
    "# NumPy Transformer Attention: Causal, KV-Cache, RoPE, GQA, and FlashAttention-Style\n",
    "\n",
    "This notebook builds a minimal-yet-complete transformer attention stack **purely in NumPy**, suitable for demos and experiments:\n",
    "\n",
    "1. **Core attention (softmax, masking)**\n",
    "2. **Causal self-attention & KV cache**\n",
    "3. **LayerNorm, MLP, Transformer block**\n",
    "4. **Rotary Positional Embeddings (RoPE)**\n",
    "5. **Grouped Query Attention (GQA)**\n",
    "6. **FlashAttention-style streaming softmax**\n",
    "7. **Integration into MHA + Decoder + simple LLM.generate**\n",
    "\n",
    "Each section includes sanity tests. You can toggle RoPE/GQA/Flash in the config section near the bottom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a66510",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdataclasses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataclass\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Any, Tuple\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109f7b4",
   "metadata": {},
   "source": [
    "## Core utilities: softmax and causal mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(x, axis=-1):\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def causal_mask(q_len, kv_len, dtype=bool):\n",
    "    \"\"\"\n",
    "    Returns a (q_len, kv_len) boolean mask where True = masked (disallowed).\n",
    "    Works for KV caching (kv_len >= q_len) and plain self-attn.\n",
    "    \"\"\"\n",
    "    shift = kv_len - q_len\n",
    "    q_idx = np.arange(q_len)[:, None]                  # (q,1)\n",
    "    k_idx = np.arange(kv_len)[None, :] - shift         # (1,kv) align last q with last k\n",
    "    return (k_idx > q_idx).astype(dtype)               # True => future (mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe973383",
   "metadata": {},
   "source": [
    "## Rotary Positional Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556032e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _rope_freqs(head_dim, seq_len, base=10000.0, dtype=np.float32):\n",
    "    assert head_dim % 2 == 0, \"RoPE requires even head_dim\"\n",
    "    half = head_dim // 2\n",
    "    inv_freq = 1.0 / (base ** (np.arange(0, half, dtype=dtype) / half))\n",
    "    t = np.arange(seq_len, dtype=dtype)[:, None] * inv_freq[None, :]  # (seq, half)\n",
    "    cos = np.cos(t); sin = np.sin(t)\n",
    "\n",
    "    cos_full = np.empty((seq_len, head_dim), dtype=dtype)\n",
    "    sin_full = np.empty((seq_len, head_dim), dtype=dtype)\n",
    "    cos_full[:, 0::2] = cos; cos_full[:, 1::2] = cos\n",
    "    sin_full[:, 0::2] = sin; sin_full[:, 1::2] = sin\n",
    "    return cos_full, sin_full\n",
    "\n",
    "def _rope_rotate_half(x):\n",
    "    x1 = x[..., 0::2]\n",
    "    x2 = x[..., 1::2]\n",
    "    out = np.empty_like(x)\n",
    "    out[..., 0::2] = -x2\n",
    "    out[..., 1::2] =  x1\n",
    "    return out\n",
    "\n",
    "def apply_rope(q, k, pos_q, pos_k=None, base=10000.0):\n",
    "    \"\"\"\n",
    "    Apply RoPE to q,k per sequence position.\n",
    "    q: (n_head, q_len, head_dim)\n",
    "    k: (n_head, kv_len, head_dim)\n",
    "    pos_q: (q_len,) integer absolute positions\n",
    "    pos_k: (kv_len,) (defaults to range(kv_len))\n",
    "    Returns rotated q,k with same shapes.\n",
    "    \"\"\"\n",
    "    n, qlen, d = q.shape\n",
    "    _, kvlen, _ = k.shape\n",
    "    if pos_k is None:\n",
    "        pos_k = np.arange(kvlen, dtype=np.int64)\n",
    "\n",
    "    cos_q, sin_q = _rope_freqs(d, qlen, base=base, dtype=q.dtype)\n",
    "    cos_k, sin_k = _rope_freqs(d, kvlen, base=base, dtype=k.dtype)\n",
    "\n",
    "    cos_q = cos_q[pos_q][None, :, :]  # (1,q,d)\n",
    "    sin_q = sin_q[pos_q][None, :, :]\n",
    "    cos_k = cos_k[pos_k][None, :, :]  # (1,kv,d)\n",
    "    sin_k = sin_k[pos_k][None, :, :]\n",
    "\n",
    "    q_rot = q * cos_q + _rope_rotate_half(q) * sin_q\n",
    "    k_rot = k * cos_k + _rope_rotate_half(k) * sin_k\n",
    "    return q_rot, k_rot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceff1df6",
   "metadata": {},
   "source": [
    "## Attention primitives: dense, FlashAttention-style, and GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d732054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def single_headed_attention(q, k, v):\n",
    "    \"\"\"\n",
    "    Single-headed causal attention with KV support.\n",
    "    q: (q, d) k: (kv, d) v: (kv, d)\n",
    "    returns: (q, d)\n",
    "    \"\"\"\n",
    "    d = q.shape[-1]\n",
    "    scores = (q @ k.T) / np.sqrt(d)                 # (q, kv)\n",
    "    mask = causal_mask(q.shape[0], k.shape[0])      # (q, kv)\n",
    "    scores = np.where(mask, -1e9, scores)\n",
    "    p = softmax(scores, axis=-1)                    # (q, kv)\n",
    "    return p @ v                                    # (q, d)\n",
    "\n",
    "def multi_headed_attention(q, k, v):\n",
    "    \"\"\"\n",
    "    q: (n, q, d)  k: (n, kv, d)  v: (n, kv, d)\n",
    "    returns: (n, q, d)\n",
    "    \"\"\"\n",
    "    n, qlen, d = q.shape\n",
    "    kvlen = k.shape[1]\n",
    "    scores = np.einsum('nqd,nkd->nqk', q, k) / np.sqrt(d)   # (n,q,kv)\n",
    "    mask = causal_mask(qlen, kvlen)[None, :, :]             # (1,q,kv)\n",
    "    scores = np.where(mask, -1e9, scores)\n",
    "    p = softmax(scores, axis=-1)\n",
    "    return np.einsum('nqk,nkd->nqd', p, v)\n",
    "\n",
    "def attention_flash(q, k, v, block_size=128):\n",
    "    \"\"\"\n",
    "    Single-head causal attention with streaming softmax (FlashAttention style).\n",
    "    q: (q, d) k: (kv, d) v: (kv, d)\n",
    "    returns: (q, d)\n",
    "    \"\"\"\n",
    "    qlen, d = q.shape\n",
    "    kvlen = k.shape[0]\n",
    "    scale = np.sqrt(d)\n",
    "\n",
    "    m = np.full((qlen,), -np.inf, dtype=q.dtype)\n",
    "    l = np.zeros((qlen,), dtype=q.dtype)\n",
    "    out = np.zeros((qlen, d), dtype=q.dtype)\n",
    "\n",
    "    shift = kvlen - qlen\n",
    "    q_idx = np.arange(qlen)[:, None]\n",
    "\n",
    "    for start in range(0, kvlen, block_size):\n",
    "        end = min(start + block_size, kvlen)\n",
    "        k_blk = k[start:end]                       # (b, d)\n",
    "        v_blk = v[start:end]                       # (b, d)\n",
    "\n",
    "        scores = (q @ k_blk.T) / scale             # (q, b)\n",
    "        k_idx_blk = (np.arange(start, end)[None, :] - shift)\n",
    "        mask = (k_idx_blk > q_idx)                 # (q, b)\n",
    "        scores = np.where(mask, -1e9, scores)\n",
    "\n",
    "        m_blk = np.max(scores, axis=-1)            # (q,)\n",
    "        m_new = np.maximum(m, m_blk)\n",
    "\n",
    "        alpha = np.exp(m - m_new)\n",
    "        beta  = np.exp(m_blk - m_new)\n",
    "\n",
    "        p_blk_unnorm = np.exp(scores - m_blk[:, None])\n",
    "        l = alpha * l + beta * np.sum(p_blk_unnorm, axis=-1)\n",
    "        out = (alpha[:, None] * out) + (p_blk_unnorm @ v_blk)\n",
    "\n",
    "        m = m_new\n",
    "\n",
    "    out = out / l[:, None]\n",
    "    return out\n",
    "\n",
    "def multi_headed_attention_flash(q, k, v, block_size=128):\n",
    "    n, qlen, d = q.shape\n",
    "    out = np.empty_like(q)\n",
    "    for h in range(n):\n",
    "        out[h] = attention_flash(q[h], k[h], v[h], block_size=block_size)\n",
    "    return out\n",
    "\n",
    "def multi_headed_attention_gqa(q, k, v):\n",
    "    \"\"\"\n",
    "    GQA causal attention.\n",
    "    q: (n_q, q, d) ; k: (n_kv, kv, d) ; v: (n_kv, kv, d)\n",
    "    Returns: (n_q, q, d)\n",
    "    \"\"\"\n",
    "    n_q, qlen, d = q.shape\n",
    "    n_kv, kvlen, _ = k.shape\n",
    "    assert n_q % n_kv == 0, \"n_q must be divisible by n_kv\"\n",
    "    group = n_q // n_kv\n",
    "    out = np.empty_like(q)\n",
    "    mask = causal_mask(qlen, kvlen)[None, :, :]\n",
    "    for kvh in range(n_kv):\n",
    "        qh_start, qh_end = kvh*group, (kvh+1)*group\n",
    "        q_slice = q[qh_start:qh_end]                 # (group,q,d)\n",
    "        k_h = k[kvh:kvh+1]                           # (1,kv,d)\n",
    "        v_h = v[kvh:kvh+1]                           # (1,kv,d)\n",
    "        scores = np.einsum('gqd,nkd->gqk', q_slice, k_h) / np.sqrt(d)\n",
    "        scores = np.where(mask, -1e9, scores)\n",
    "        p = softmax(scores, axis=-1)\n",
    "        out[qh_start:qh_end] = np.einsum('gqk,nkd->gqd', p, v_h)\n",
    "    return out\n",
    "\n",
    "def multi_headed_attention_flash_gqa(q, k, v, block_size=128):\n",
    "    n_q, qlen, d = q.shape\n",
    "    n_kv, kvlen, _ = k.shape\n",
    "    assert n_q % n_kv == 0\n",
    "    group = n_q // n_kv\n",
    "    out = np.empty_like(q)\n",
    "    for kvh in range(n_kv):\n",
    "        qh_start, qh_end = kvh*group, (kvh+1)*group\n",
    "        for h in range(qh_start, qh_end):\n",
    "            out[h] = attention_flash(q[h], k[kvh], v[kvh], block_size=block_size)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7925a9",
   "metadata": {},
   "source": [
    "## Split/Merge heads and basic layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f98f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_heads(x, n_head):\n",
    "    # x: (seq, hidden) -> (n_head, seq, head_dim)\n",
    "    seq, hidden = x.shape\n",
    "    d = hidden // n_head\n",
    "    return np.transpose(x.reshape(seq, n_head, d), (1,0,2))\n",
    "\n",
    "def merge_heads(x):\n",
    "    # x: (n_head, seq, head_dim) -> (seq, hidden)\n",
    "    n, seq, d = x.shape\n",
    "    return np.transpose(x, (1,0,2)).reshape(seq, n*d)\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "def layer_norm(x, g, b, eps: float = 1e-5):\n",
    "    mu = np.mean(x, axis=-1, keepdims=True)\n",
    "    var = np.var(x, axis=-1, keepdims=True)\n",
    "    return g * (x - mu) / np.sqrt(var + eps) + b\n",
    "\n",
    "def linear(x, w, b):\n",
    "    return x @ w + b\n",
    "\n",
    "def ffn(x, c_fc, c_proj):\n",
    "    return linear(gelu(linear(x, **c_fc)), **c_proj)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e274272",
   "metadata": {},
   "source": [
    "## Multi-Head Attention (MHA) with flags: RoPE, GQA, Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde94aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class AttnConfig:\n",
    "    n_head: int\n",
    "    n_kv_head: int = None         # if None => standard MHA; else => GQA\n",
    "    use_rope: bool = False\n",
    "    rope_base: float = 10000.0\n",
    "    use_flash: bool = False\n",
    "    flash_block: int = 128\n",
    "\n",
    "def mha_dispatch(q, k_cache, v_cache, c_attn, c_proj, attn_cfg: AttnConfig, pos_offset: int = 0):\n",
    "    \"\"\"\n",
    "    x-projection, KV-cache update, split heads, optional RoPE, and attention variant.\n",
    "    q: (seq, hidden)  (this is the input x to the attn block, already pre-LN)\n",
    "    k_cache/v_cache: (kv, hidden) caches for this layer\n",
    "    returns: (seq, hidden), k_cache, v_cache\n",
    "    \"\"\"\n",
    "    x = linear(q, **c_attn)                     # (seq, 3H)\n",
    "    q_, k_, v_ = np.split(x, 3, axis=-1)        # (seq,H) each\n",
    "\n",
    "    # update caches\n",
    "    k_cache = np.concatenate([k_cache, k_], axis=0)\n",
    "    v_cache = np.concatenate([v_cache, v_], axis=0)\n",
    "\n",
    "    if attn_cfg.n_kv_head is None:\n",
    "        # standard MHA\n",
    "        qh = split_heads(q_, attn_cfg.n_head)               # (n, q, d)\n",
    "        kh = split_heads(k_cache, attn_cfg.n_head)          # (n, kv, d)\n",
    "        vh = split_heads(v_cache, attn_cfg.n_head)          # (n, kv, d)\n",
    "        if attn_cfg.use_rope:\n",
    "            q_pos = np.arange(pos_offset, pos_offset + qh.shape[1], dtype=np.int64)\n",
    "            k_pos = np.arange(0, kh.shape[1], dtype=np.int64)\n",
    "            qh, kh = apply_rope(qh, kh, q_pos, k_pos, base=attn_cfg.rope_base)\n",
    "        if attn_cfg.use_flash:\n",
    "            ah = multi_headed_attention_flash(qh, kh, vh, block_size=attn_cfg.flash_block)\n",
    "        else:\n",
    "            ah = multi_headed_attention(qh, kh, vh)\n",
    "        out = merge_heads(ah)\n",
    "    else:\n",
    "        # GQA\n",
    "        n_kv = attn_cfg.n_kv_head\n",
    "        n_q = attn_cfg.n_head\n",
    "        qh = split_heads(q_, n_q)                            # (n_q,q,d)\n",
    "        kh = split_heads(k_cache, n_kv)                      # (n_kv,kv,d)\n",
    "        vh = split_heads(v_cache, n_kv)                      # (n_kv,kv,d)\n",
    "        if attn_cfg.use_rope:\n",
    "            q_pos = np.arange(pos_offset, pos_offset + qh.shape[1], dtype=np.int64)\n",
    "            k_pos = np.arange(0, kh.shape[1], dtype=np.int64)\n",
    "            # Rotate q and k independently\n",
    "            qh, _ = apply_rope(qh, qh, q_pos, q_pos, base=attn_cfg.rope_base)\n",
    "            _, kh = apply_rope(kh, kh, k_pos, k_pos, base=attn_cfg.rope_base)\n",
    "        if attn_cfg.use_flash:\n",
    "            ah = multi_headed_attention_flash_gqa(qh, kh, vh, block_size=attn_cfg.flash_block)\n",
    "        else:\n",
    "            ah = multi_headed_attention_gqa(qh, kh, vh)\n",
    "        out = merge_heads(ah)\n",
    "\n",
    "    out = linear(out, **c_proj)\n",
    "    return out, k_cache, v_cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddafb0d",
   "metadata": {},
   "source": [
    "## Transformer Block and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d400bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transformer_block(x, mlp, attn, ln_1, ln_2, attn_cfg: AttnConfig, k_cache, v_cache, pos_offset: int):\n",
    "    a, k_cache, v_cache = mha_dispatch(\n",
    "        q=layer_norm(x, **ln_1),\n",
    "        k_cache=k_cache, v_cache=v_cache,\n",
    "        c_attn=attn['c_attn'], c_proj=attn['c_proj'],\n",
    "        attn_cfg=attn_cfg, pos_offset=pos_offset\n",
    "    )\n",
    "    x = x + a\n",
    "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
    "    return x, k_cache, v_cache\n",
    "\n",
    "def decoder(input_ids, seq_len, wte, wpe, blocks, ln_f, attn_cfg: AttnConfig, k_cache, v_cache):\n",
    "    if isinstance(input_ids, list):  # prompt pass\n",
    "        x = wte[input_ids] + wpe[np.arange(len(input_ids))]\n",
    "    else:  # single token generation\n",
    "        x = wte[[input_ids]] + wpe[[seq_len - 1]]\n",
    "\n",
    "    # pos_offset is absolute position of first token in this x chunk\n",
    "    # If input_ids is list, it's positioned at 0..len-1.\n",
    "    # If scalar, it is at position seq_len-1.\n",
    "    pos_offset = 0 if isinstance(input_ids, list) else (seq_len - 1)\n",
    "\n",
    "    for i, block in enumerate(blocks):\n",
    "        x, k_cache_i, v_cache_i = transformer_block(\n",
    "            x, mlp=block['mlp'], attn=block['attn'],\n",
    "            ln_1=block['ln_1'], ln_2=block['ln_2'],\n",
    "            attn_cfg=attn_cfg,\n",
    "            k_cache=k_cache[i], v_cache=v_cache[i],\n",
    "            pos_offset=pos_offset\n",
    "        )\n",
    "        k_cache[i] = k_cache_i\n",
    "        v_cache[i] = v_cache_i\n",
    "\n",
    "    logits = layer_norm(x, **ln_f) @ wte.T\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2243a65",
   "metadata": {},
   "source": [
    "## Tiny tokenizer and random parameters (for demo purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c626d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TinyTokenizer:\n",
    "    def __init__(self):\n",
    "        # simple charset (extend as needed)\n",
    "        chars = ['<pad>','<unk>'] +                 list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,!?'-\")\n",
    "        self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "        self.itos = {i:ch for ch,i in self.stoi.items()}\n",
    "    def encode(self, s: str):\n",
    "        return [self.stoi.get(ch, 1) for ch in s]\n",
    "    def decode(self, ids: List[int]):\n",
    "        return ''.join(self.itos.get(i, '?') for i in ids)\n",
    "\n",
    "def init_params(vocab_size, hidden_dim, max_ctx, n_layer, n_head, n_kv_head=None, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    params = {}\n",
    "    params['wte'] = rng.normal(0, 0.02, size=(vocab_size, hidden_dim)).astype(np.float32)\n",
    "    params['wpe'] = rng.normal(0, 0.02, size=(max_ctx, hidden_dim)).astype(np.float32)\n",
    "\n",
    "    blocks = []\n",
    "    for _ in range(n_layer):\n",
    "        c_attn = {\n",
    "            'w': rng.normal(0, 0.02, size=(hidden_dim, 3*hidden_dim)).astype(np.float32),\n",
    "            'b': np.zeros((3*hidden_dim,), dtype=np.float32),\n",
    "        }\n",
    "        c_proj = {\n",
    "            'w': rng.normal(0, 0.02, size=(hidden_dim, hidden_dim)).astype(np.float32),\n",
    "            'b': np.zeros((hidden_dim,), dtype=np.float32),\n",
    "        }\n",
    "        c_fc = {\n",
    "            'w': rng.normal(0, 0.02, size=(hidden_dim, 4*hidden_dim)).astype(np.float32),\n",
    "            'b': np.zeros((4*hidden_dim,), dtype=np.float32),\n",
    "        }\n",
    "        c_mlp_proj = {\n",
    "            'w': rng.normal(0, 0.02, size=(4*hidden_dim, hidden_dim)).astype(np.float32),\n",
    "            'b': np.zeros((hidden_dim,), dtype=np.float32),\n",
    "        }\n",
    "        ln_1 = {'g': np.ones((hidden_dim,), dtype=np.float32), 'b': np.zeros((hidden_dim,), dtype=np.float32)}\n",
    "        ln_2 = {'g': np.ones((hidden_dim,), dtype=np.float32), 'b': np.zeros((hidden_dim,), dtype=np.float32)}\n",
    "        blocks.append({\n",
    "            'attn': {'c_attn': c_attn, 'c_proj': c_proj},\n",
    "            'mlp': {'c_fc': c_fc, 'c_proj': c_mlp_proj},\n",
    "            'ln_1': ln_1,\n",
    "            'ln_2': ln_2,\n",
    "        })\n",
    "    ln_f = {'g': np.ones((hidden_dim,), dtype=np.float32), 'b': np.zeros((hidden_dim,), dtype=np.float32)}\n",
    "\n",
    "    return {'wte': params['wte'], 'wpe': params['wpe'], 'blocks': blocks, 'ln_f': ln_f}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c19750",
   "metadata": {},
   "source": [
    "## LLM wrapper with KV cache and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f37aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLM:\n",
    "    def __init__(self, tokenizer, params, model_config, attn_cfg: AttnConfig):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.params = params\n",
    "        self.model_config = model_config\n",
    "        self.attn_cfg = attn_cfg\n",
    "\n",
    "    def _init_cache(self):\n",
    "        num_layers = self.model_config['n_layer']\n",
    "        hidden_dim = self.model_config['n_embd']\n",
    "        k_cache = [np.zeros((0, hidden_dim), dtype=np.float32) for _ in range(num_layers)]\n",
    "        v_cache = [np.zeros((0, hidden_dim), dtype=np.float32) for _ in range(num_layers)]\n",
    "        return k_cache, v_cache\n",
    "\n",
    "    def forward_prompt(self, input_ids):\n",
    "        k_cache, v_cache = self._init_cache()\n",
    "        probs = decoder(\n",
    "            input_ids, len(input_ids),\n",
    "            wte=self.params['wte'], wpe=self.params['wpe'],\n",
    "            blocks=self.params['blocks'], ln_f=self.params['ln_f'],\n",
    "            attn_cfg=self.attn_cfg, k_cache=k_cache, v_cache=v_cache\n",
    "        )\n",
    "        return probs, k_cache, v_cache\n",
    "\n",
    "    def generate(self, prompt, max_new_tokens=20, greedy=True):\n",
    "        input_ids = self.tokenizer.encode(prompt)\n",
    "        # prime caches with prompt\n",
    "        _, k_cache, v_cache = self.forward_prompt(input_ids)\n",
    "\n",
    "        output_ids = []\n",
    "        seq_len = len(input_ids)\n",
    "        for _ in tqdm(range(max_new_tokens), desc='Generating', dynamic_ncols=True):\n",
    "            last_id = input_ids[-1] if len(output_ids)==0 else output_ids[-1]\n",
    "            probs = decoder(\n",
    "                last_id, seq_len + 1,\n",
    "                wte=self.params['wte'], wpe=self.params['wpe'],\n",
    "                blocks=self.params['blocks'], ln_f=self.params['ln_f'],\n",
    "                attn_cfg=self.attn_cfg, k_cache=k_cache, v_cache=v_cache\n",
    "            )\n",
    "            next_id = int(np.argmax(probs[-1])) if greedy else int(np.random.choice(probs.shape[-1], p=probs[-1]))\n",
    "            output_ids.append(next_id)\n",
    "            seq_len += 1\n",
    "        return self.tokenizer.decode(output_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21e871e",
   "metadata": {},
   "source": [
    "## Sanity tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd107117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _attn_dense(q, k, v):\n",
    "    scores = (q @ k.T) / np.sqrt(q.shape[-1])\n",
    "    scores = np.where(causal_mask(q.shape[0], k.shape[0]), -1e9, scores)\n",
    "    p = softmax(scores, axis=-1)\n",
    "    return p @ v\n",
    "\n",
    "def run_tests():\n",
    "    rng = np.random.default_rng(0)\n",
    "\n",
    "    # softmax\n",
    "    x = np.array([[1., 2., 3.], [1000., 1000., 1000.]], dtype=np.float32)\n",
    "    s = softmax(x, axis=-1)\n",
    "    assert np.allclose(s.sum(-1), 1.0)\n",
    "\n",
    "    # causal mask\n",
    "    assert causal_mask(3,3).shape == (3,3)\n",
    "    assert causal_mask(1,5).shape == (1,5)\n",
    "    assert causal_mask(5,1).shape == (5,1)\n",
    "\n",
    "    # single-head + flash equivalence\n",
    "    q = rng.normal(size=(7, 16)).astype(np.float32)\n",
    "    k = rng.normal(size=(23, 16)).astype(np.float32)\n",
    "    v = rng.normal(size=(23, 16)).astype(np.float32)\n",
    "    d_dense  = _attn_dense(q, k, v)\n",
    "    d_flash  = attention_flash(q, k, v, block_size=6)\n",
    "    assert np.allclose(d_dense, d_flash, atol=1e-5)\n",
    "\n",
    "    # multi-head\n",
    "    qh = rng.normal(size=(4, 5, 16)).astype(np.float32)\n",
    "    kh = rng.normal(size=(4, 9, 16)).astype(np.float32)\n",
    "    vh = rng.normal(size=(4, 9, 16)).astype(np.float32)\n",
    "    o  = multi_headed_attention(qh, kh, vh)\n",
    "    of = multi_headed_attention_flash(qh, kh, vh, block_size=4)\n",
    "    assert o.shape == of.shape == (4,5,16)\n",
    "\n",
    "    # GQA\n",
    "    qg = rng.normal(size=(8, 5, 16)).astype(np.float32)\n",
    "    kg = rng.normal(size=(2, 9, 16)).astype(np.float32)\n",
    "    vg = rng.normal(size=(2, 9, 16)).astype(np.float32)\n",
    "    og  = multi_headed_attention_gqa(qg, kg, vg)\n",
    "    ogf = multi_headed_attention_flash_gqa(qg, kg, vg, block_size=4)\n",
    "    assert og.shape == ogf.shape == (8,5,16)\n",
    "\n",
    "    # RoPE shapes\n",
    "    qh2 = rng.normal(size=(4, 6, 16)).astype(np.float32)\n",
    "    kh2 = rng.normal(size=(4, 10, 16)).astype(np.float32)\n",
    "    qrot, krot = apply_rope(qh2, kh2, np.arange(6), np.arange(10))\n",
    "    assert qrot.shape == qh2.shape and krot.shape == kh2.shape\n",
    "\n",
    "    # split/merge\n",
    "    X = rng.normal(size=(7, 32)).astype(np.float32)\n",
    "    assert np.allclose(X, merge_heads(split_heads(X, 4)))\n",
    "\n",
    "    print(\"All tests passed âœ…\")\n",
    "\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc863973",
   "metadata": {},
   "source": [
    "## Configure a tiny model and try generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46bbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model config\n",
    "model_config = {\n",
    "    'vocab_size': 80,   # from TinyTokenizer size (will compute below)\n",
    "    'n_embd': 32,\n",
    "    'max_ctx': 128,\n",
    "    'n_layer': 2,\n",
    "    'n_head': 4,\n",
    "    # 'n_kv_head': 2,    # uncomment to enable GQA (n_head must be divisible by n_kv_head)\n",
    "}\n",
    "\n",
    "# Build tokenizer to get vocab size\n",
    "tok = TinyTokenizer()\n",
    "model_config['vocab_size'] = len(tok.stoi)\n",
    "\n",
    "# Init random params\n",
    "params = init_params(\n",
    "    vocab_size=model_config['vocab_size'],\n",
    "    hidden_dim=model_config['n_embd'],\n",
    "    max_ctx=model_config['max_ctx'],\n",
    "    n_layer=model_config['n_layer'],\n",
    "    n_head=model_config['n_head'],\n",
    ")\n",
    "\n",
    "# Attention config toggles\n",
    "attn_cfg = AttnConfig(\n",
    "    n_head=model_config['n_head'],\n",
    "    n_kv_head=None,          # set to 2 to enable GQA with 4 query heads -> 2 kv heads\n",
    "    use_rope=True,           # toggle RoPE\n",
    "    rope_base=10000.0,\n",
    "    use_flash=True,          # toggle FlashAttention-style\n",
    "    flash_block=32\n",
    ")\n",
    "\n",
    "# Instantiate model\n",
    "llm = LLM(tok, params, model_config, attn_cfg)\n",
    "\n",
    "# Try generation (random weights => babble)\n",
    "prompt = \"Hello\"\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"---- Generating (greedy) ----\")\n",
    "print(llm.generate(prompt, max_new_tokens=40, greedy=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

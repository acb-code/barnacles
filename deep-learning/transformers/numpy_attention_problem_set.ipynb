{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bd57afb6",
      "metadata": {
        "id": "bd57afb6"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
        "https://colab.research.google.com/github/acb-code/barnacles/blob/main/deep-learning/transformers/numpy_attention_problem_set.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a863d56",
      "metadata": {
        "id": "6a863d56"
      },
      "source": [
        "\n",
        "# NumPy Attention Problem Set (with Solutions)\n",
        "\n",
        "This problem set walks you through implementing core transformer attention components in **NumPy**, one at a time.\n",
        "For each section you get:\n",
        "1) **Math & description** (what to build)  \n",
        "2) **Implementation shell** with a `TODO`  \n",
        "3) **Public tests** that must pass  \n",
        "4) **Hidden solution** (collapsed/hidden by default)\n",
        "\n",
        "> In Colab: use **View → Show/hide code** or the cell \"...\" menu to toggle hidden solution cells.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb3a8e7",
      "metadata": {
        "id": "0cb3a8e7"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7e3756f",
      "metadata": {
        "id": "b7e3756f"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def almost_equal(a, b, tol=1e-6):\n",
        "    return np.allclose(a, b, atol=tol, rtol=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a01f1953",
      "metadata": {
        "id": "a01f1953"
      },
      "source": [
        "\n",
        "## 1. Softmax (stable)\n",
        "**Goal.** Implement a numerically stable softmax over a specified axis.\n",
        "\n",
        "**Math.**\n",
        "For vector \\(x\\), softmax is\n",
        "\\[ \\mathrm{softmax}(x)_i = \\frac{e^{x_i - \\max_j x_j}}{\\sum_j e^{x_j - \\max_k x_k}}. \\]\n",
        "\n",
        "**Why shift by max?** To avoid overflow when \\(x\\) has large magnitude.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ab7dc8d",
      "metadata": {
        "id": "0ab7dc8d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement softmax(x, axis=-1) with numerical stability.\n",
        "def softmax(x, axis=-1):\n",
        "    \"\"\"Return softmax(x) along `axis` using a max-shift for numerical stability.\"\"\"\n",
        "    # TODO: shift by max along axis, exponentiate, normalize.\n",
        "    # Replace the next line with your implementation.\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4709c675",
      "metadata": {
        "id": "4709c675"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests (do not modify)\n",
        "x = np.array([[1., 2., 3.],\n",
        "              [1000., 1000., 1000.]], dtype=np.float64)\n",
        "s = None\n",
        "try:\n",
        "    s = softmax(x, axis=-1)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"softmax() not implemented yet.\")\n",
        "assert s.shape == x.shape\n",
        "row_sums = s.sum(axis=-1)\n",
        "assert np.allclose(row_sums, 1.0), \"Rows must sum to 1.\"\n",
        "print(\"Softmax tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d977fb32",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "d977fb32"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def softmax(x, axis=-1):\n",
        "    x = x - np.max(x, axis=axis, keepdims=True)\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "print(\"Softmax solution loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f5d0e37",
      "metadata": {
        "id": "2f5d0e37"
      },
      "source": [
        "\n",
        "## 2. Causal Mask\n",
        "**Goal.** Return a boolean mask of shape `(q_len, kv_len)` where **True means masked** (future positions).  \n",
        "Should work when `kv_len >= q_len` (KV-cache).\n",
        "\n",
        "**Idea.** Align the last query with the last key by a shift, then mask `k > q`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8183db00",
      "metadata": {
        "id": "8183db00"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement causal_mask(q_len, kv_len) -> (q_len, kv_len) boolean mask.\n",
        "def causal_mask(q_len, kv_len, dtype=bool):\n",
        "    \"\"\"True where attending would violate causality (future).\"\"\"\n",
        "    # Hint: build query indices (q,1) and key indices (1,kv) with an alignment shift = kv_len - q_len\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d61e476",
      "metadata": {
        "id": "1d61e476"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests\n",
        "m = None\n",
        "try:\n",
        "    m = causal_mask(3, 5)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"causal_mask() not implemented yet.\")\n",
        "assert m.shape == (3,5)\n",
        "# Check last row (q=2) can see up to last key: only \"future\" masked\n",
        "# Align last q (2) to last k (4): allow k<=4 and not greater than 2 after alignment\n",
        "# Quick sanity: no row should be all True\n",
        "assert np.all(~np.all(m, axis=-1)), \"No row should be fully masked.\"\n",
        "print(\"Causal mask tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed34050",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "eed34050"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def causal_mask(q_len, kv_len, dtype=bool):\n",
        "    shift = kv_len - q_len\n",
        "    q_idx = np.arange(q_len)[:, None]\n",
        "    k_idx = np.arange(kv_len)[None, :] - shift\n",
        "    return (k_idx > q_idx).astype(dtype)\n",
        "print(\"Causal mask solution loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0363b5bc",
      "metadata": {
        "id": "0363b5bc"
      },
      "source": [
        "\n",
        "## 3. Single-Head Causal Attention (with KV support)\n",
        "**Goal.** Implement:\n",
        "\\[ \\mathrm{Attn}(Q,K,V) = \\mathrm{softmax}(\\frac{QK^\\top}{\\sqrt{d}} + M)\\;V, \\]\n",
        "where \\(M\\) applies a large negative value to masked positions (use the boolean causal mask).\n",
        "\n",
        "**Shapes.**\n",
        "- `q`: `(q, d)`\n",
        "- `k`: `(kv, d)`\n",
        "- `v`: `(kv, d)`\n",
        "- return: `(q, d)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41204532",
      "metadata": {
        "id": "41204532"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement single_headed_attention(q, k, v)\n",
        "def single_headed_attention(q, k, v):\n",
        "    d = q.shape[-1]\n",
        "    # 1) scores = q @ k.T / sqrt(d)\n",
        "    # 2) mask with causal_mask(q, kv) -> set masked to large negative\n",
        "    # 3) p = softmax(scores, axis=-1)\n",
        "    # 4) return p @ v\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9d073a",
      "metadata": {
        "id": "ed9d073a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests\n",
        "rng = np.random.default_rng(0)\n",
        "q = rng.normal(size=(3, 8)).astype(np.float64)\n",
        "k = rng.normal(size=(7, 8)).astype(np.float64)\n",
        "v = rng.normal(size=(7, 8)).astype(np.float64)\n",
        "\n",
        "try:\n",
        "    out = single_headed_attention(q, k, v)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"single_headed_attention() not implemented yet.\")\n",
        "\n",
        "assert out.shape == (3, 8)\n",
        "# Check numerics: should be finite\n",
        "assert np.isfinite(out).all()\n",
        "print(\"Single-head attention tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db08bb5",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "4db08bb5"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def single_headed_attention(q, k, v):\n",
        "    d = q.shape[-1]\n",
        "    scores = (q @ k.T) / np.sqrt(d)\n",
        "    mask = causal_mask(q.shape[0], k.shape[0])\n",
        "    scores = np.where(mask, -1e9, scores)\n",
        "    p = softmax(scores, axis=-1)\n",
        "    return p @ v\n",
        "print(\"Single-head attention solution loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08827405",
      "metadata": {
        "id": "08827405"
      },
      "source": [
        "\n",
        "## 4. Split/Merge Heads\n",
        "**Goal.** Convert between `(seq, hidden)` and `(n_head, seq, head_dim)` where `hidden = n_head * head_dim`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341a61f7",
      "metadata": {
        "id": "341a61f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement split_heads(x, n_head) and merge_heads(x)\n",
        "def split_heads(x, n_head):\n",
        "    # x: (seq, hidden) -> (n_head, seq, head_dim)\n",
        "    raise NotImplementedError\n",
        "\n",
        "def merge_heads(x):\n",
        "    # x: (n_head, seq, head_dim) -> (seq, hidden)\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f8e5b6",
      "metadata": {
        "id": "64f8e5b6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests\n",
        "rng = np.random.default_rng(1)\n",
        "X = rng.normal(size=(5, 24)).astype(np.float64)\n",
        "S = None\n",
        "try:\n",
        "    S = split_heads(X, 3)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"split_heads() not implemented yet.\")\n",
        "assert S.shape == (3, 5, 8)\n",
        "M = None\n",
        "try:\n",
        "    M = merge_heads(S)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"merge_heads() not implemented yet.\")\n",
        "assert M.shape == X.shape and np.allclose(M, X)\n",
        "print(\"Split/Merge tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4404c58",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "e4404c58"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def split_heads(x, n_head):\n",
        "    seq, hidden = x.shape\n",
        "    d = hidden // n_head\n",
        "    return np.transpose(x.reshape(seq, n_head, d), (1,0,2))\n",
        "\n",
        "def merge_heads(x):\n",
        "    n, seq, d = x.shape\n",
        "    return np.transpose(x, (1,0,2)).reshape(seq, n*d)\n",
        "print(\"Split/Merge solutions loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e880bb98",
      "metadata": {
        "id": "e880bb98"
      },
      "source": [
        "\n",
        "## 5. Multi-Head Causal Attention (dense version)\n",
        "**Goal.** Use `split_heads` and `merge_heads`, compute per-head attention, and combine.\n",
        "\n",
        "**Shapes.**\n",
        "- `q`: `(n, q, d)`\n",
        "- `k`: `(n, kv, d)`\n",
        "- `v`: `(n, kv, d)`\n",
        "- return: `(n, q, d)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b521349",
      "metadata": {
        "id": "7b521349"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement multi_headed_attention(q, k, v) using einsum & causal mask.\n",
        "def multi_headed_attention(q, k, v):\n",
        "    # scores = einsum('nqd, nkd -> nqk') / sqrt(d)\n",
        "    # apply mask (broadcast (1,q,kv)), softmax over -1, then out = einsum('nqk, nkd -> nqd')\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458ea213",
      "metadata": {
        "id": "458ea213"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests\n",
        "rng = np.random.default_rng(2)\n",
        "qh = rng.normal(size=(4, 5, 8)).astype(np.float64)\n",
        "kh = rng.normal(size=(4, 9, 8)).astype(np.float64)\n",
        "vh = rng.normal(size=(4, 9, 8)).astype(np.float64)\n",
        "\n",
        "try:\n",
        "    oh = multi_headed_attention(qh, kh, vh)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"multi_headed_attention() not implemented yet.\")\n",
        "\n",
        "assert oh.shape == (4, 5, 8)\n",
        "assert np.isfinite(oh).all()\n",
        "print(\"MHA (dense) tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30004f72",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "30004f72"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def multi_headed_attention(q, k, v):\n",
        "    n, qlen, d = q.shape\n",
        "    kvlen = k.shape[1]\n",
        "    scores = np.einsum('nqd,nkd->nqk', q, k) / np.sqrt(d)\n",
        "    mask = causal_mask(qlen, kvlen)[None, :, :]\n",
        "    scores = np.where(mask, -1e9, scores)\n",
        "    p = softmax(scores, axis=-1)\n",
        "    return np.einsum('nqk,nkd->nqd', p, v)\n",
        "print(\"MHA (dense) solution loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e72bd08",
      "metadata": {
        "id": "0e72bd08"
      },
      "source": [
        "\n",
        "## 6. Linear, GELU, LayerNorm, and FFN\n",
        "**Goal.** Implement basic layers used in transformer blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83d9aa0e",
      "metadata": {
        "id": "83d9aa0e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODOs: linear, gelu, layer_norm, ffn\n",
        "def linear(x, w, b):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def gelu(x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def ffn(x, c_fc, c_proj):\n",
        "    # c_fc: {w:(H,4H), b:(4H,)}, c_proj: {w:(4H,H), b:(H,)}\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290ee71e",
      "metadata": {
        "id": "290ee71e"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests\n",
        "rng = np.random.default_rng(3)\n",
        "x = rng.normal(size=(4, 6)).astype(np.float64)\n",
        "w = rng.normal(size=(6, 10)).astype(np.float64)\n",
        "b = np.zeros((10,), dtype=np.float64)\n",
        "try:\n",
        "    y = linear(x, w, b)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"linear() not implemented yet.\")\n",
        "assert y.shape == (4, 10)\n",
        "print(\"Linear test passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892f13ba",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "892f13ba"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def linear(x, w, b):\n",
        "    return x @ w + b\n",
        "\n",
        "def gelu(x):\n",
        "    return 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
        "\n",
        "def layer_norm(x, g, b, eps: float = 1e-5):\n",
        "    mu = np.mean(x, axis=-1, keepdims=True)\n",
        "    var = np.var(x, axis=-1, keepdims=True)\n",
        "    return g * (x - mu) / np.sqrt(var + eps) + b\n",
        "\n",
        "def ffn(x, c_fc, c_proj):\n",
        "    return linear(gelu(linear(x, **c_fc)), **c_proj)\n",
        "print(\"Basic layers solutions loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7451e2fd",
      "metadata": {
        "id": "7451e2fd"
      },
      "source": [
        "\n",
        "## 7. FlashAttention-style (single-head streaming softmax)\n",
        "**Goal.** Implement a numerically-stable streaming attention that processes keys/values in blocks and maintains running max/sum.\n",
        "\n",
        "**Hint.** Maintain per-row running `m` (max), `l` (sum of exp), and `out` (numerator), updating them per block.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fbd8652",
      "metadata": {
        "id": "6fbd8652"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement attention_flash(q, k, v, block_size=128)\n",
        "def attention_flash(q, k, v, block_size=128):\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fc1a522",
      "metadata": {
        "id": "3fc1a522"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests: compare to dense attention for equality (within tolerance)\n",
        "def _attn_dense(q, k, v):\n",
        "    scores = (q @ k.T) / np.sqrt(q.shape[-1])\n",
        "    scores = np.where(causal_mask(q.shape[0], k.shape[0]), -1e9, scores)\n",
        "    p = softmax(scores, axis=-1)\n",
        "    return p @ v\n",
        "\n",
        "rng = np.random.default_rng(4)\n",
        "q = rng.normal(size=(7, 16)).astype(np.float64)\n",
        "k = rng.normal(size=(23, 16)).astype(np.float64)\n",
        "v = rng.normal(size=(23, 16)).astype(np.float64)\n",
        "\n",
        "try:\n",
        "    of = attention_flash(q, k, v, block_size=6)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"attention_flash() not implemented yet.\")\n",
        "\n",
        "od = _attn_dense(q, k, v)\n",
        "assert np.allclose(od, of, atol=1e-5)\n",
        "print(\"FlashAttention-style tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42cc888a",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "42cc888a"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def attention_flash(q, k, v, block_size=128):\n",
        "    qlen, d = q.shape\n",
        "    kvlen = k.shape[0]\n",
        "    scale = np.sqrt(d)\n",
        "\n",
        "    m = np.full((qlen,), -np.inf, dtype=q.dtype)\n",
        "    l = np.zeros((qlen,), dtype=q.dtype)\n",
        "    out = np.zeros((qlen, d), dtype=q.dtype)\n",
        "\n",
        "    shift = kvlen - qlen\n",
        "    q_idx = np.arange(qlen)[:, None]\n",
        "\n",
        "    for start in range(0, kvlen, block_size):\n",
        "        end = min(start + block_size, kvlen)\n",
        "        k_blk = k[start:end]\n",
        "        v_blk = v[start:end]\n",
        "\n",
        "        scores = (q @ k_blk.T) / scale\n",
        "        k_idx_blk = (np.arange(start, end)[None, :] - shift)\n",
        "        mask = (k_idx_blk > q_idx)\n",
        "        scores = np.where(mask, -1e9, scores)\n",
        "\n",
        "        m_blk = np.max(scores, axis=-1)\n",
        "        m_new = np.maximum(m, m_blk)\n",
        "\n",
        "        alpha = np.exp(m - m_new)\n",
        "        beta  = np.exp(m_blk - m_new)\n",
        "\n",
        "        p_blk_unnorm = np.exp(scores - m_blk[:, None])\n",
        "        l = alpha * l + beta * np.sum(p_blk_unnorm, axis=-1)\n",
        "        out = (alpha[:, None] * out) + (p_blk_unnorm @ v_blk)\n",
        "\n",
        "        m = m_new\n",
        "\n",
        "    return out / l[:, None]\n",
        "print(\"FlashAttention-style solution loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e67877c",
      "metadata": {
        "id": "0e67877c"
      },
      "source": [
        "\n",
        "## 8. Rotary Positional Embeddings (RoPE)\n",
        "**Goal.** Implement RoPE helpers to rotate even/odd pairs by position-dependent angles.\n",
        "\n",
        "**Requirements.** `head_dim` is even; implement `_rope_freqs`, `_rope_rotate_half`, and `apply_rope`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e7a6a1d",
      "metadata": {
        "id": "3e7a6a1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODOs: RoPE helpers\n",
        "def _rope_freqs(head_dim, seq_len, base=10000.0, dtype=np.float64):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def _rope_rotate_half(x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "def apply_rope(q, k, pos_q, pos_k=None, base=10000.0):\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d18edb2d",
      "metadata": {
        "id": "d18edb2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests\n",
        "rng = np.random.default_rng(5)\n",
        "n, qlen, kvlen, d = 4, 3, 5, 8\n",
        "q = rng.normal(size=(n, qlen, d))\n",
        "k = rng.normal(size=(n, kvlen, d))\n",
        "try:\n",
        "    qrot, krot = apply_rope(q, k, np.arange(qlen), np.arange(kvlen))\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"apply_rope() not implemented yet.\")\n",
        "assert qrot.shape == q.shape and krot.shape == k.shape\n",
        "print(\"RoPE tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0643b83e",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "0643b83e"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def _rope_freqs(head_dim, seq_len, base=10000.0, dtype=np.float64):\n",
        "    assert head_dim % 2 == 0\n",
        "    half = head_dim // 2\n",
        "    inv = 1.0 / (base ** (np.arange(0, half, dtype=dtype) / half))\n",
        "    t = np.arange(seq_len, dtype=dtype)[:, None] * inv[None, :]\n",
        "    cos = np.cos(t); sin = np.sin(t)\n",
        "    cos_full = np.empty((seq_len, head_dim), dtype=dtype)\n",
        "    sin_full = np.empty((seq_len, head_dim), dtype=dtype)\n",
        "    cos_full[:, 0::2] = cos; cos_full[:, 1::2] = cos\n",
        "    sin_full[:, 0::2] = sin; sin_full[:, 1::2] = sin\n",
        "    return cos_full, sin_full\n",
        "\n",
        "def _rope_rotate_half(x):\n",
        "    x1 = x[..., 0::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    out = np.empty_like(x)\n",
        "    out[..., 0::2] = -x2\n",
        "    out[..., 1::2] =  x1\n",
        "    return out\n",
        "\n",
        "def apply_rope(q, k, pos_q, pos_k=None, base=10000.0):\n",
        "    n, qlen, d = q.shape\n",
        "    _, kvlen, _ = k.shape\n",
        "    if pos_k is None:\n",
        "        pos_k = np.arange(kvlen, dtype=np.int64)\n",
        "\n",
        "    cos_q, sin_q = _rope_freqs(d, qlen, base=base, dtype=q.dtype)\n",
        "    cos_k, sin_k = _rope_freqs(d, kvlen, base=base, dtype=k.dtype)\n",
        "\n",
        "    cos_q = cos_q[pos_q][None, :, :]\n",
        "    sin_q = sin_q[pos_q][None, :, :]\n",
        "    cos_k = cos_k[pos_k][None, :, :]\n",
        "    sin_k = sin_k[pos_k][None, :, :]\n",
        "\n",
        "    q_rot = q * cos_q + _rope_rotate_half(q) * sin_q\n",
        "    k_rot = k * cos_k + _rope_rotate_half(k) * sin_k\n",
        "    return q_rot, k_rot\n",
        "print(\"RoPE solutions loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e11a6a1",
      "metadata": {
        "id": "5e11a6a1"
      },
      "source": [
        "\n",
        "## 9. Grouped-Query Attention (GQA)\n",
        "**Goal.** Implement MHA where there are fewer KV heads than Q heads (`n_q % n_kv == 0`).  \n",
        "Map groups of query heads to a single KV head.\n",
        "\n",
        "**Return.** Shape `(n_q, q, d)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3c59957",
      "metadata": {
        "id": "a3c59957"
      },
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Implement multi_headed_attention_gqa(q, k, v)\n",
        "def multi_headed_attention_gqa(q, k, v):\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f1974b",
      "metadata": {
        "id": "04f1974b"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Tests\n",
        "rng = np.random.default_rng(6)\n",
        "q = rng.normal(size=(8, 5, 16))\n",
        "k = rng.normal(size=(2, 9, 16))\n",
        "v = rng.normal(size=(2, 9, 16))\n",
        "\n",
        "try:\n",
        "    o = multi_headed_attention_gqa(q, k, v)\n",
        "except NotImplementedError:\n",
        "    raise AssertionError(\"multi_headed_attention_gqa() not implemented yet.\")\n",
        "assert o.shape == (8, 5, 16)\n",
        "print(\"GQA tests passed ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47dc6774",
      "metadata": {
        "tags": [
          "solution"
        ],
        "cellView": "form",
        "id": "47dc6774"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "# --- Solution (hidden) ---\n",
        "def multi_headed_attention_gqa(q, k, v):\n",
        "    n_q, qlen, d = q.shape\n",
        "    n_kv, kvlen, _ = k.shape\n",
        "    assert n_q % n_kv == 0\n",
        "    group = n_q // n_kv\n",
        "    out = np.empty_like(q)\n",
        "    mask = causal_mask(qlen, kvlen)[None, :, :]\n",
        "    for kvh in range(n_kv):\n",
        "        qh_start, qh_end = kvh*group, (kvh+1)*group\n",
        "        q_slice = q[qh_start:qh_end]  # (group,q,d)\n",
        "        k_h = k[kvh:kvh+1]           # (1,kv,d)\n",
        "        v_h = v[kvh:kvh+1]           # (1,kv,d)\n",
        "        scores = np.einsum('gqd,nkd->gqk', q_slice, k_h) / np.sqrt(d)\n",
        "        scores = np.where(mask, -1e9, scores)\n",
        "        p = softmax(scores, axis=-1)\n",
        "        out[qh_start:qh_end] = np.einsum('gqk,nkd->gqd', p, v_h)\n",
        "    return out\n",
        "print(\"GQA solution loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88abebc4",
      "metadata": {
        "id": "88abebc4"
      },
      "source": [
        "\n",
        "## 10. MHA Dispatch (Flags: RoPE, GQA, Flash)\n",
        "**Goal.** Write a dispatcher that:\n",
        "1) Projects to QKV via `c_attn` (linear)  \n",
        "2) Updates KV cache by concatenation  \n",
        "3) Splits heads (either standard MHA or GQA)  \n",
        "4) Optionally applies RoPE to q/k  \n",
        "5) Runs attention via dense or Flash variant  \n",
        "6) Merges heads and projects with `c_proj`\n",
        "\n",
        "*(For brevity, here we provide a tested reference solution below.)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "414af39a",
      "metadata": {
        "tags": [
          "solution"
        ],
        "id": "414af39a"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Reference Solution (hidden) ---\n",
        "def mha_dispatch(q, k_cache, v_cache, c_attn, c_proj,\n",
        "                 n_head, n_kv_head=None, use_rope=False, rope_base=10000.0,\n",
        "                 use_flash=False, flash_block=128, pos_offset=0):\n",
        "    x = linear(q, **c_attn)                     # (seq, 3H)\n",
        "    q_, k_, v_ = np.split(x, 3, axis=-1)\n",
        "\n",
        "    k_cache = np.concatenate([k_cache, k_], axis=0)\n",
        "    v_cache = np.concatenate([v_cache, v_], axis=0)\n",
        "\n",
        "    if n_kv_head is None:\n",
        "        qh = split_heads(q_, n_head)\n",
        "        kh = split_heads(k_cache, n_head)\n",
        "        vh = split_heads(v_cache, n_head)\n",
        "        if use_rope:\n",
        "            q_pos = np.arange(pos_offset, pos_offset + qh.shape[1], dtype=np.int64)\n",
        "            k_pos = np.arange(0, kh.shape[1], dtype=np.int64)\n",
        "            qh, kh = apply_rope(qh, kh, q_pos, k_pos, base=rope_base)\n",
        "        if use_flash:\n",
        "            ah = np.empty_like(qh)\n",
        "            for h in range(qh.shape[0]):\n",
        "                ah[h] = attention_flash(qh[h], kh[h], vh[h], block_size=flash_block)\n",
        "        else:\n",
        "            ah = multi_headed_attention(qh, kh, vh)\n",
        "        out = merge_heads(ah)\n",
        "    else:\n",
        "        qh = split_heads(q_, n_head)\n",
        "        kh = split_heads(k_cache, n_kv_head)\n",
        "        vh = split_heads(v_cache, n_kv_head)\n",
        "        if use_rope:\n",
        "            q_pos = np.arange(pos_offset, pos_offset + qh.shape[1], dtype=np.int64)\n",
        "            k_pos = np.arange(0, kh.shape[1], dtype=np.int64)\n",
        "            # apply independently\n",
        "            qh, _ = apply_rope(qh, qh, q_pos, q_pos, base=rope_base)\n",
        "            _, kh = apply_rope(kh, kh, k_pos, k_pos, base=rope_base)\n",
        "        if use_flash:\n",
        "            ah = np.empty_like(qh)\n",
        "            assert n_head % n_kv_head == 0\n",
        "            group = n_head // n_kv_head\n",
        "            for kvh in range(n_kv_head):\n",
        "                qh_start, qh_end = kvh*group, (kvh+1)*group\n",
        "                for h in range(qh_start, qh_end):\n",
        "                    ah[h] = attention_flash(qh[h], kh[kvh], vh[kvh], block_size=flash_block)\n",
        "        else:\n",
        "            ah = multi_headed_attention_gqa(qh, kh, vh)\n",
        "        out = merge_heads(ah)\n",
        "\n",
        "    out = linear(out, **c_proj)\n",
        "    return out, k_cache, v_cache\n",
        "print(\"MHA dispatch reference loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d33c7e11",
      "metadata": {
        "id": "d33c7e11"
      },
      "source": [
        "\n",
        "## 11. Transformer Block (Pre-Norm)\n",
        "**Goal.** Compose LN → MHA → residual, then LN → FFN → residual.  \n",
        "A reference solution is provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d4d42f2",
      "metadata": {
        "tags": [
          "solution"
        ],
        "id": "9d4d42f2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Reference Solution (hidden) ---\n",
        "def transformer_block(x, mlp, attn, ln_1, ln_2, cfg, k_cache, v_cache, pos_offset):\n",
        "    a, k_cache, v_cache = mha_dispatch(\n",
        "        q=layer_norm(x, **ln_1),\n",
        "        k_cache=k_cache, v_cache=v_cache,\n",
        "        c_attn=attn['c_attn'], c_proj=attn['c_proj'],\n",
        "        n_head=cfg['n_head'],\n",
        "        n_kv_head=cfg.get('n_kv_head', None),\n",
        "        use_rope=cfg.get('use_rope', False),\n",
        "        rope_base=cfg.get('rope_base', 10000.0),\n",
        "        use_flash=cfg.get('use_flash', False),\n",
        "        flash_block=cfg.get('flash_block', 128),\n",
        "        pos_offset=pos_offset\n",
        "    )\n",
        "    x = x + a\n",
        "    x = x + ffn(layer_norm(x, **ln_2), **mlp)\n",
        "    return x, k_cache, v_cache\n",
        "print(\"Transformer block reference loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c413c86f",
      "metadata": {
        "id": "c413c86f"
      },
      "source": [
        "\n",
        "## 12. Decoder\n",
        "**Goal.** Build a minimal decoder with tied embeddings and per-layer KV caches.  \n",
        "A reference solution is provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec846cd3",
      "metadata": {
        "tags": [
          "solution"
        ],
        "id": "ec846cd3"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Reference Solution (hidden) ---\n",
        "def decoder(input_ids, seq_len, wte, wpe, blocks, ln_f, cfg, k_cache, v_cache):\n",
        "    if isinstance(input_ids, list):\n",
        "        x = wte[input_ids] + wpe[np.arange(len(input_ids))]\n",
        "        pos_offset = 0\n",
        "    else:\n",
        "        x = wte[[input_ids]] + wpe[[seq_len - 1]]\n",
        "        pos_offset = seq_len - 1\n",
        "\n",
        "    for i, block in enumerate(blocks):\n",
        "        x, k_cache_i, v_cache_i = transformer_block(\n",
        "            x, mlp=block['mlp'], attn=block['attn'],\n",
        "            ln_1=block['ln_1'], ln_2=block['ln_2'],\n",
        "            cfg=cfg, k_cache=k_cache[i], v_cache=v_cache[i],\n",
        "            pos_offset=pos_offset\n",
        "        )\n",
        "        k_cache[i] = k_cache_i\n",
        "        v_cache[i] = v_cache_i\n",
        "\n",
        "    logits = layer_norm(x, **ln_f) @ wte.T\n",
        "    probs = softmax(logits, axis=-1)\n",
        "    return probs\n",
        "print(\"Decoder reference loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c5b18f7",
      "metadata": {
        "id": "0c5b18f7"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### You’ve reached the end of the core exercises!\n",
        "From here, you can wire these components together into an end-to-end demo.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}